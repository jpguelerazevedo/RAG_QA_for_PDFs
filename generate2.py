import os
import json
import tiktoken # Para tokeniza√ß√£o e chunking
import re # Importar m√≥dulo para express√µes regulares
import ollama

# --- Configura√ß√µes ---
EMBEDDING_MODEL = "nomic-embed-text:latest"
INPUT_MD_PATH = "./mdPath/pdf2.md"
EMBEDDINGS_OUTPUT_PATH = "./embeddings/embeddings.json"
CHUNK_SIZE = 512 # Tamanho de cada peda√ßo de texto em tokens (para texto n√£o-tabela)
OVERLAP_SIZE = 128 #     Sobreposi√ß√£o entre os peda√ßos (para texto n√£o-tabela)

# --- Fun√ß√£o para gerar embedding via Ollama ---
def generate_embedding(text: str):
    """Gera embedding usando o modelo nomic-embed-text via Ollama."""
    try:
        response = ollama.embeddings(model=EMBEDDING_MODEL, prompt=text)
        return response['embedding']
    except Exception as e:
        print(f"Erro ao gerar embedding: {e}")
        return None

# --- Fun√ß√£o para dividir texto por se√ß√µes baseadas em cabe√ßalhos ## ---
def chunk_text_by_sections(text, max_chunk_size=2048, encoding_name="cl100k_base"):
    """
    Divide o texto em se√ß√µes baseadas nos cabe√ßalhos ## (n√≠vel 2).
    Cada par√°grafo dentro de uma se√ß√£o ter√° seu pr√≥prio embedding.
    
    Args:
        text: Texto a ser dividido
        max_chunk_size: Tamanho m√°ximo do chunk em tokens (para par√°grafos muito grandes)
        encoding_name: Nome do encoding para tokeniza√ß√£o
        
    Returns:
        list: Lista de dicion√°rios com par√°grafos e contexto
    """
    tokenizer = tiktoken.get_encoding(encoding_name)
    
    # Dividir o texto em linhas
    lines = text.split('\n')
    sections = []
    current_section = ""
    current_title = ""
    
    for line in lines:
        line_stripped = line.strip()
        
        # Verificar se √© um cabe√ßalho de n√≠vel 2 (##)
        if line_stripped.startswith('## ') and not line_stripped.startswith('### '):
            # Se j√° temos uma se√ß√£o em andamento, salv√°-la
            if current_section.strip():
                sections.append({
                    "title": current_title,
                    "content": current_section.strip()
                })
            
            # Come√ßar nova se√ß√£o
            current_title = line_stripped[3:].strip()  # Remover "## "
            current_section = line + '\n'
        else:
            # Adicionar linha √† se√ß√£o atual
            current_section += line + '\n'
    
    # Adicionar a √∫ltima se√ß√£o
    if current_section.strip():
        sections.append({
            "title": current_title,
            "content": current_section.strip()
        })
    
    # Processar cada se√ß√£o para criar embeddings por par√°grafo
    section_chunks = []
    for section in sections:
        section_content = section["content"]
        section_title = section["title"]
        
        # Dividir a se√ß√£o em par√°grafos (separados por linha dupla)
        paragraphs = re.split(r'\n\s*\n', section_content)
        
        for i, paragraph in enumerate(paragraphs):
            paragraph = paragraph.strip()
            if paragraph:  # Ignorar par√°grafos vazios
                # Verificar se o par√°grafo cont√©m apenas o cabe√ßalho
                lines = paragraph.split('\n')
                is_header_only = (len(lines) == 1 and lines[0].startswith('## '))
                
                # Calcular tokens do par√°grafo
                paragraph_tokens = len(tokenizer.encode(paragraph))
                
                # Se √© apenas cabe√ßalho, pular este par√°grafo
                if is_header_only:
                    continue
                
                # Se o par√°grafo √© muito grande, dividi-lo em partes menores
                if paragraph_tokens > max_chunk_size:
                    # Dividir par√°grafo em chunks menores
                    sub_chunks = chunk_text_with_overlap(paragraph, CHUNK_SIZE, OVERLAP_SIZE, encoding_name)
                    
                    for j, sub_chunk in enumerate(sub_chunks):
                        # Remover o cabe√ßalho se estiver presente no in√≠cio do sub_chunk
                        content_without_header = sub_chunk
                        if content_without_header.startswith('## '):
                            lines_sub = content_without_header.split('\n', 1)
                            if len(lines_sub) > 1:
                                content_without_header = lines_sub[1].strip()
                            else:
                                # Se s√≥ tem cabe√ßalho, pular
                                continue
                        
                        contextual_content = f"[Se√ß√£o: {section_title}] {content_without_header}"
                        
                        section_chunks.append({
                            "type": "text_chunk",
                            "content": sub_chunk,
                            "contextual_content": contextual_content,
                            "section_title": section_title,
                            "chunk_index": j,
                            "total_chunks": len(sub_chunks)
                        })
                else:
                    # Par√°grafo pequeno o suficiente para ser um chunk √∫nico
                    # Remover o cabe√ßalho se estiver presente no in√≠cio do par√°grafo
                    content_without_header = paragraph
                    if content_without_header.startswith('## '):
                        lines_para = content_without_header.split('\n', 1)
                        if len(lines_para) > 1:
                            content_without_header = lines_para[1].strip()
                        else:
                            # Se s√≥ tem cabe√ßalho, pular
                            continue
                    
                    contextual_content = f"[Se√ß√£o: {section_title}] {content_without_header}"
                    
                    section_chunks.append({
                        "type": "text_chunk",
                        "content": paragraph,
                        "contextual_content": contextual_content,
                        "section_title": section_title,
                        "chunk_index": 0,
                        "total_chunks": 1
                    })
    
    return section_chunks

# --- Fun√ß√£o para dividir texto com sobreposi√ß√£o ---
def chunk_text_with_overlap(text, chunk_size, overlap_size, encoding_name="cl100k_base"):
    """
    Divide o texto em chunks com sobreposi√ß√£o entre eles.
    
    Args:
        text: Texto a ser dividido
        chunk_size: Tamanho do chunk em tokens
        overlap_size: Tamanho da sobreposi√ß√£o em tokens
        encoding_name: Nome do encoding para tokeniza√ß√£o
        
    Returns:
        list: Lista de chunks de texto
    """
    tokenizer = tiktoken.get_encoding(encoding_name)
    tokens = tokenizer.encode(text)
    
    chunks = []
    start_idx = 0
    
    while start_idx < len(tokens):
        # Definir fim do chunk
        end_idx = start_idx + chunk_size
        
        # Extrair tokens do chunk
        chunk_tokens = tokens[start_idx:end_idx]
        
        # Decodificar de volta para texto
        chunk_text = tokenizer.decode(chunk_tokens)
        chunks.append(chunk_text)
        
        # Mover para o pr√≥ximo chunk com sobreposi√ß√£o
        start_idx = end_idx - overlap_size
        
        # Se chegamos ao fim, parar
        if end_idx >= len(tokens):
            break
    
    return chunks

# --- Fun√ß√£o para extrair tabelas do markdown ---
def extract_tables_from_markdown(text):
    """
    Extrai tabelas do markdown e retorna uma lista de dicion√°rios com informa√ß√µes sobre cada tabela.
    
    Args:
        text: Texto markdown completo
        
    Returns:
        list: Lista de dicion√°rios com informa√ß√µes sobre tabelas
    """
    # Regex para encontrar tabelas markdown
    table_pattern = r'(\|[^|\n]*\|[^|\n]*\|[^\n]*\n)(\|[-\s|]*\|[^\n]*\n)(((\|[^|\n]*\|[^|\n]*\|[^\n]*\n)+))'
    
    tables = []
    matches = re.finditer(table_pattern, text, re.MULTILINE)
    
    for match in matches:
        table_text = match.group(0)
        
        # Separar cabe√ßalho, separador e linhas
        lines = table_text.strip().split('\n')
        
        if len(lines) >= 3:  # Pelo menos cabe√ßalho, separador e uma linha
            header = lines[0]
            separator = lines[1]
            data_lines = lines[2:]
            
            # Extrair colunas do cabe√ßalho
            header_columns = [col.strip() for col in header.split('|') if col.strip()]
            
            # Processar cada linha de dados
            for i, line in enumerate(data_lines):
                if line.strip():  # Ignorar linhas vazias
                    columns = [col.strip() for col in line.split('|') if col.strip()]
                    
                    # Criar conte√∫do da linha
                    row_content = ' | '.join(columns)
                    
                    # Criar conte√∫do contextual sem repetir o cabe√ßalho
                    contextual_content = row_content
                    
                    tables.append({
                        "type": "table_row",
                        "content": row_content,
                        "contextual_content": contextual_content,
                        "table_header": header_columns,
                        "table_header_raw": header,
                        "row_number": i + 1,
                        "row_data": columns
                    })
    
    return tables

# --- Fun√ß√£o para remover tabelas do texto ---
def remove_tables_from_text(text):
    """
    Remove tabelas do texto markdown, mantendo apenas o texto puro.
    
    Args:
        text: Texto markdown com tabelas
        
    Returns:
        str: Texto sem tabelas
    """
    # Regex para encontrar e remover tabelas
    table_pattern = r'(\|[^|\n]*\|[^|\n]*\|[^\n]*\n)(\|[-\s|]*\|[^\n]*\n)(((\|[^|\n]*\|[^|\n]*\|[^\n]*\n)+))'
    text_without_tables = re.sub(table_pattern, '', text, flags=re.MULTILINE)
    
    # Limpar linhas vazias extras
    text_without_tables = re.sub(r'\n\s*\n\s*\n', '\n\n', text_without_tables)
    
    return text_without_tables

# --- Fun√ß√£o principal ---
def main():
    """
    Fun√ß√£o principal que processa o arquivo markdown e gera embeddings.
    """
    # Verificar se o arquivo de entrada existe
    if not os.path.exists(INPUT_MD_PATH):
        print(f"Erro: Arquivo de entrada '{INPUT_MD_PATH}' n√£o encontrado.")
        return
    
    # Ler o arquivo markdown
    print(f"Lendo arquivo markdown: {INPUT_MD_PATH}")
    with open(INPUT_MD_PATH, 'r', encoding='utf-8') as f:
        markdown_content = f.read()
    
    print(f"Arquivo lido com sucesso. Tamanho: {len(markdown_content)} caracteres")
    
    # Extrair tabelas
    print("Extraindo tabelas do markdown...")
    tables = extract_tables_from_markdown(markdown_content)
    print(f"Encontradas {len(tables)} linhas de tabela")
    
    # Remover tabelas do texto para processar texto puro
    print("Removendo tabelas do texto...")
    text_without_tables = remove_tables_from_text(markdown_content)
    
    # Dividir texto em se√ß√µes
    print("Dividindo texto em se√ß√µes...")
    text_chunks = chunk_text_by_sections(text_without_tables)
    print(f"Criados {len(text_chunks)} chunks de texto")
    
    # Combinar todos os itens para embedding
    all_items = text_chunks + tables
    print(f"Total de itens para embedding: {len(all_items)}")
    
    # Gerar embeddings
    print("Gerando embeddings...")
    embeddings_data = []
    
    for i, item in enumerate(all_items):
        print(f"Processando item {i+1}/{len(all_items)}: {item['type']}")
        
        # Usar conte√∫do contextual para gerar embedding
        text_for_embedding = item.get('contextual_content', item['content'])
        
        # Gerar embedding
        embedding = generate_embedding(text_for_embedding)
        
        if embedding is not None:
            # Adicionar ao resultado
            embeddings_data.append({
                "id": i,
                "type": item["type"],
                "content": item["content"],
                "contextual_content": item.get("contextual_content", ""),
                "embedding": embedding,
                **{k: v for k, v in item.items() if k not in ["type", "content", "contextual_content"]}
            })
        else:
            print(f"Erro ao gerar embedding para item {i+1}. Pulando.")
    
    # Salvar embeddings
    print(f"Salvando embeddings em {EMBEDDINGS_OUTPUT_PATH}")
    os.makedirs(os.path.dirname(EMBEDDINGS_OUTPUT_PATH), exist_ok=True)
    
    with open(EMBEDDINGS_OUTPUT_PATH, 'w', encoding='utf-8') as f:
        json.dump(embeddings_data, f, ensure_ascii=False, indent=2)
    
    print(f"Embeddings salvos com sucesso!")
    print(f"Total de embeddings gerados: {len(embeddings_data)}")
    
    # Estat√≠sticas
    text_chunks_count = len([item for item in embeddings_data if item["type"] == "text_chunk"])
    table_rows_count = len([item for item in embeddings_data if item["type"] == "table_row"])
    
    print(f"\nüìä Estat√≠sticas:")
    print(f"   - Chunks de texto: {text_chunks_count}")
    print(f"   - Linhas de tabela: {table_rows_count}")
    print(f"   - Total: {len(embeddings_data)}")
    
    # Verificar dimens√µes dos embeddings
    if embeddings_data:
        embedding_dim = len(embeddings_data[0]['embedding'])
        print(f"   - Dimens√£o dos embeddings: {embedding_dim}")

if __name__ == "__main__":
    main()
