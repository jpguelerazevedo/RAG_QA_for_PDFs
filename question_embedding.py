import os
import json
import numpy as np
import faiss
import sys # Para sa√≠da de erro mais controlada
import ollama

# --- Configura√ß√µes Principais ---
# Use vari√°veis de ambiente ou um arquivo de configura√ß√£o para op√ß√µes mais flex√≠veis
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "nomic-embed-text:latest")
EMBEDDINGS_JSON_PATH = os.getenv("EMBEDDINGS_DATA_PATH", "./embeddings/embeddings.json")
# QUERY_TEXT pode vir de uma entrada do usu√°rio em um sistema real
# Por enquanto, mantemos aqui para teste
QUERY_TEXT = "qual √© o per√≠do de isen√ß√£o da taxa ?" 
NUM_RESULTS = 15

# --- Fun√ß√µes Auxiliares ---
def calculate_cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """Calcula a similaridade de cosseno entre dois vetores numpy."""
    dot_product = np.dot(vec1, vec2)
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)
    
    if norm_vec1 == 0 or norm_vec2 == 0:
        return 0.0 # Evita divis√£o por zero para vetores nulos
        
    return dot_product / (norm_vec1 * norm_vec2)

# --- Classes/Fun√ß√µes de L√≥gica de Neg√≥cio ---

class EmbeddingGenerator:
    """Gerencia o carregamento do modelo e a gera√ß√£o de embeddings via Ollama."""
    def __init__(self, model_name: str = "nomic-embed-text:latest"):
        self.model_name = model_name
        self._test_model()

    def _test_model(self):
        """Testa se o modelo est√° dispon√≠vel no Ollama."""
        try:
            print(f"Testando modelo de embedding: {self.model_name}")
            # Teste simples para verificar se o modelo est√° dispon√≠vel
            test_response = ollama.embeddings(model=self.model_name, prompt="teste")
            print("Modelo de embedding carregado com sucesso via Ollama.")
        except Exception as e:
            print(f"Erro ao acessar o modelo via Ollama: {e}", file=sys.stderr)
            print(f"Certifique-se de que o modelo '{self.model_name}' est√° instalado no Ollama.", file=sys.stderr)
            print("Execute: ollama pull nomic-embed-text:latest", file=sys.stderr)
            sys.exit(1)

    def generate_embedding(self, text: str) -> np.ndarray:
        """Gera o vetor de embedding para um dado texto via Ollama."""
        try:
            response = ollama.embeddings(model=self.model_name, prompt=text)
            return np.array(response['embedding'], dtype='float32')
        except Exception as e:
            print(f"Erro ao gerar embedding para o texto: '{text[:50]}...': {e}", file=sys.stderr)
            return None

class DocumentEmbeddingsDB:
    """Gerencia o carregamento e indexa√ß√£o dos embeddings de documentos."""
    def __init__(self, embeddings_json_path: str):
        self.embeddings_json_path = embeddings_json_path
        self.document_data = [] # Para armazenar ID, texto e vetor raw
        self.document_vectors = None # Apenas os vetores para FAISS
        self.document_texts = []
        self.document_ids = []
        self.document_types = []  # Para armazenar o tipo de cada embedding
        self.faiss_index = None
        self._load_and_index_embeddings()

    def _load_and_index_embeddings(self):
        """Carrega os embeddings do JSON e cria o √≠ndice FAISS."""
        if not os.path.exists(self.embeddings_json_path):
            print(f"Erro: O arquivo de embeddings '{self.embeddings_json_path}' n√£o foi encontrado.", file=sys.stderr)
            print("Certifique-se de que voc√™ j√° gerou os embeddings do PDF usando o script anterior.", file=sys.stderr)
            sys.exit(1)

        try:
            with open(self.embeddings_json_path, 'r', encoding='utf-8') as f:
                self.document_data = json.load(f)
            print(f"Embeddings do PDF carregados com sucesso de '{self.embeddings_json_path}'.")

            if not self.document_data:
                print("Aviso: O arquivo de embeddings est√° vazio. N√£o h√° dados para comparar.")
                sys.exit(0) # Sai sem erro se n√£o h√° dados para processar

            # Separar os dados para uso mais eficiente
            self.document_vectors = np.array([item['embedding'] for item in self.document_data], dtype='float32')
            
            # Adaptar para a nova estrutura de dados com contextual_content
            self.document_texts = []
            self.document_ids = []
            self.document_types = []
            
            for item in self.document_data:
                self.document_ids.append(item['id'])
                self.document_types.append(item['type'])
                
                # Para embeddings de linha de tabela, usar o conte√∫do contextual
                if item['type'] == 'table_row':
                    # Usar o conte√∫do contextual que √© mais informativo para busca
                    display_text = f"[TABELA] {item['contextual_content']}"
                    self.document_texts.append(display_text)
                else:
                    # Para chunks de texto, usar o conte√∫do contextual se dispon√≠vel
                    if 'contextual_content' in item and item['contextual_content']:
                        self.document_texts.append(item['contextual_content'])
                    else:
                        self.document_texts.append(item['content'])

            # Criar e adicionar ao √≠ndice FAISS
            embedding_dimension = self.document_vectors.shape[1]
            self.faiss_index = faiss.IndexFlatL2(embedding_dimension)
            print(f"Adicionando {self.document_vectors.shape[0]} vetores ao √≠ndice FAISS...")
            self.faiss_index.add(self.document_vectors)
            print("Vetores adicionados ao √≠ndice FAISS.")
            
            # Exibir estat√≠sticas
            stats = self.get_statistics()
            print(f"\nüìä Estat√≠sticas dos embeddings:")
            print(f"   - Total: {stats['total_embeddings']} embeddings")
            print(f"   - Chunks de texto: {stats['text_chunks']}")
            print(f"   - Linhas de tabela: {stats['table_rows']}")
            for tipo, count in stats['types'].items():
                if tipo not in ['text_chunk', 'table_row']:
                    print(f"   - {tipo}: {count}")
            print()

        except json.JSONDecodeError as e:
            print(f"Erro ao decodificar JSON do arquivo '{self.embeddings_json_path}': {e}", file=sys.stderr)
            sys.exit(1)
        except Exception as e:
            print(f"Erro ao ler ou processar o arquivo de embeddings '{self.embeddings_json_path}': {e}", file=sys.stderr)
            sys.exit(1)

    def search_similar(self, query_vector: np.ndarray, num_results: int) -> list:
        """
        Realiza a busca de similaridade no √≠ndice FAISS.
        Retorna uma lista de dicion√°rios com 'id', 'text', 'type', 'distance_l2', 'cosine_similarity'.
        """
        if self.faiss_index is None or self.document_vectors is None:
            print("Erro: √çndice FAISS ou vetores de documento n√£o foram inicializados.", file=sys.stderr)
            return []

        # Certifica-se de que o vetor de consulta √© 2D
        query_vector_2d = query_vector.reshape(1, -1) 
        
        # Buscar resultados
        search_results = min(num_results * 2, len(self.document_data))  # Buscar 2x mais para ter op√ß√µes
        distances_l2, indices = self.faiss_index.search(query_vector_2d, search_results)

        results = []
        if len(indices[0]) == 0:
            return [] # Nenhuma similaridade encontrada

        for i in range(len(indices[0])): # Itera sobre os resultados reais, n√£o num_results fixo
            doc_index = indices[0][i]
            distance_l2 = distances_l2[0][i]

            # Obter o vetor original do documento para c√°lculo da similaridade de cosseno
            document_vector_raw = self.document_data[doc_index]['embedding']
            cosine_similarity = calculate_cosine_similarity(query_vector, np.array(document_vector_raw, dtype='float32'))
            
            # Obter informa√ß√µes adicionais do documento original
            original_item = self.document_data[doc_index]
            
            result = {
                "id": self.document_ids[doc_index],
                "text": self.document_texts[doc_index],
                "type": self.document_types[doc_index],
                "distance_l2": distance_l2,
                "cosine_similarity": cosine_similarity,
                "original_content": original_item.get('content', ''),
            }
            
            # Adicionar campos espec√≠ficos para linhas de tabela
            if original_item['type'] == 'table_row':
                result.update({
                    "contextual_content": original_item.get('contextual_content', ''),
                    "table_header": original_item.get('table_header_raw', ''),
                    "row_number": original_item.get('row_number', 0)
                })
            elif original_item['type'] == 'text_chunk':
                result.update({
                    "section_title": original_item.get('section_title', ''),
                    "contextual_content": original_item.get('contextual_content', '')
                })
            
            results.append(result)
        
        # Ordenar por similaridade de cosseno e retornar apenas o n√∫mero solicitado
        results.sort(key=lambda x: x['cosine_similarity'], reverse=True)
        return results[:num_results]

    def get_statistics(self):
        """Retorna estat√≠sticas sobre os embeddings carregados."""
        if not self.document_data:
            return {}
        
        stats = {
            'total_embeddings': len(self.document_data),
            'text_chunks': 0,
            'table_rows': 0,
            'types': {}
        }
        
        for item in self.document_data:
            item_type = item.get('type', 'unknown')
            stats['types'][item_type] = stats['types'].get(item_type, 0) + 1
            
            if item_type == 'text_chunk':
                stats['text_chunks'] += 1
            elif item_type == 'table_row':
                stats['table_rows'] += 1
        
        return stats

# --- Fun√ß√£o Principal para Orquestrar ---
def main():
    # 1. Carregar o gerador de embeddings
    embedder = EmbeddingGenerator(EMBEDDING_MODEL)

    # 2. Carregar e indexar os embeddings dos documentos
    db = DocumentEmbeddingsDB(EMBEDDINGS_JSON_PATH)

    # 3. Gerar o embedding da pergunta
    print(f"\nGerando embedding para a pergunta: '{QUERY_TEXT}'...")
    query_vector = embedder.generate_embedding(QUERY_TEXT)
    if query_vector is None:
        print("N√£o foi poss√≠vel gerar o embedding da pergunta. Encerrando.", file=sys.stderr)
        sys.exit(1)
    print("Embedding da pergunta gerado com sucesso.")

    # 4. Buscar os chunks mais similares
    similar_chunks = db.search_similar(query_vector, NUM_RESULTS)

    # 5. Imprimir os resultados
    print(f"\n--- Top {NUM_RESULTS} Chunks mais similares para a pergunta ---")
    if not similar_chunks:
        print("Nenhum resultado de similaridade encontrado.")
    else:
        for i, result in enumerate(similar_chunks):
            print(f"\n{'='*60}")
            print(f"Resultado {i+1}:")
            print(f"  Chunk ID original: {result['id']}")
            print(f"  Tipo: {result['type']}")
            print(f"  Dist√¢ncia Euclidiana (L2): {result['distance_l2']:.4f}")
            print(f"  Similaridade de Cosseno: {result['cosine_similarity']:.4f} (1.0 = id√™ntico, 0.0 = sem rela√ß√£o)")
            
            # Exibir informa√ß√µes espec√≠ficas para linhas de tabela
            if result['type'] == 'table_row':
                print(f"  [LINHA DE TABELA - Linha {result['row_number']}]")
                print(f"  Conte√∫do contextual: {result['contextual_content']}")
                print(f"  Conte√∫do original: {result['original_content']}")
            elif result['type'] == 'text_chunk':
                print(f"  [SE√á√ÉO DE TEXTO]")
                if result.get('section_title'):
                    print(f"  T√≠tulo da se√ß√£o: {result['section_title']}")
                print(f"  Conte√∫do:")
                print("  " + "-" * 50)
                # Limitar o texto para n√£o sobrecarregar a sa√≠da
                text_display = result['text'][:500] + "..." if len(result['text']) > 500 else result['text']
                print(f"  {text_display}")
                print("  " + "-" * 50)
            else:
                print(f"  Conte√∫do:")
                print("  " + "-" * 50)
                text_display = result['text'][:500] + "..." if len(result['text']) > 500 else result['text']
                print(f"  {text_display}")
                print("  " + "-" * 50)

    print("\nProcesso conclu√≠do.")

# --- Executar a fun√ß√£o principal ---
if __name__ == "__main__":
    main()